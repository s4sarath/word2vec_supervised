{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "STOPWORDS = list(set(STOPWORDS))\n",
    "hand_picked_stop_words = ['rt' , \"it's\" , 'says' , \"doesn't\" , \"shes\" ,\"hes\" , \"she's\" ,\"he's\", u\"don't\" , \"thanks\" , \"thank's\", \"like\" ,\n",
    "                         \"today\" , \"time\", \"know\" , \"knows\" , \"help\" , \"check\" , \"good\", 'must', 'back', 'service' ,\n",
    "                         'trust', 'yesterday' , 'before' , 'away', 'products', \"we're\", \"bad\", \"its\" ,\"it's\" , \"like\" ,\n",
    "                         'tell', 'talk' , 'wait', 'think','thinks','00pm',\"jr's\", 'truth','want','wants', 'give','gave',\n",
    "                         'sure', 'edit', 'may', 'maybe', 'may not', 'might','might not', \"we've\", 'able', 'go', 'goes',\n",
    "                         'went', \"what's\", 'list', 'lists', \"can't\", 'forever', 'ever', 'says', 'item', \"we'd\", '#deporthillary',\n",
    "                         'woud', 'will', 'would', 'mmmmk', \"t'was\", \"ira's\", 'sehe', 'haaa', \"l'art\", 'spss', \"bryan's\"]\n",
    "STOPWORDS = STOPWORDS + hand_picked_stop_words\n",
    "import re\n",
    "import numpy as np\n",
    "import graphlab as gl\n",
    "from utils.vector_utils import find_norm , find_similar , vector_from_document_tfidf_dict , vector_from_document_tfidf_mongo\n",
    "from utils.preprocess import pre_process_sentence , pre_process_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from twitter_preprocess import tweet_parser , deduplication_tweets , preprocess_tweets_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_model = gl.load_model('../Data/topic_model_v_1.0_gl/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_me_the_topic(tweet):\n",
    "    \n",
    "    if isinstance(tweet, list):\n",
    "        tweets_deduplicated = deduplication_tweets(tweet)\n",
    "        tweet_parsed = tweet_parser(tweets_deduplicated)\n",
    "        tweet_list = [t.split() for t in tweet_parsed]\n",
    "        tweet_preprocess_list = map(preprocess_tweets_test , tweet_list)\n",
    "        ######### Remove any null tweets\n",
    "        final_tweets = []\n",
    "        tweet_index = []\n",
    "        for i,t in enumerate(tweet_preprocess_list):\n",
    "            if t:\n",
    "                final_tweets.append(t)\n",
    "                tweet_index.append(i)\n",
    "        sf_tweet = gl.SFrame(final_tweets)\n",
    "        tweet_docs = gl.text_analytics.count_words(sf_tweet['X1'])\n",
    "        return tweets_deduplicated , tweet_docs , tweet_index , final_tweets\n",
    "    \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "path = '../Data/twitter_categ_data/'\n",
    "data_holder_dict = {}\n",
    "for infile in glob.glob( os.path.join(path, '*.json') ):\n",
    "    data_holder_dict[infile.split('/')[-1].split('.')[0]] = json.load(open(infile))\n",
    "    print \"current file is: \" + infile\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Health & Fitness',\n",
       " 'Business',\n",
       " 'Government',\n",
       " 'Telecom',\n",
       " 'automobile',\n",
       " 'Travel',\n",
       " 'Sports',\n",
       " 'Food & Drink',\n",
       " 'Personal Finance',\n",
       " 'Arts&Entertainment',\n",
       " 'Real Estate',\n",
       " 'technology & computing',\n",
       " 'Shopping',\n",
       " 'Classifieds',\n",
       " 'Style&Fashion',\n",
       " 'Charity']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_holder_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_holder = []\n",
    "for key_ in data_holder_dict.iterkeys():\n",
    "    sub_key_ = data_holder_dict[key_].keys()[0]\n",
    "    tweet_data = data_holder_dict[key_][sub_key_]\n",
    "    for tweet_dict in tweet_data:\n",
    "        tweet_holder.append(tweet_dict[u'twitter_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_holder = []\n",
    "\n",
    "tweet_data = data_holder_dict['automobile']['automobile']\n",
    "for tweet_dict in tweet_data:\n",
    "    tweet_holder.append(tweet_dict[u'twitter_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TD , tweet_holder_processed , TI , tweet_preprocess = get_me_the_topic(tweet_holder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype: int\n",
       "Rows: 1683\n",
       "[3, 14, 18, 3, 18, 3, 19, 7, 14, 19, 14, 19, 19, 14, 3, 17, 3, 20, 18, 3, 18, 18, 3, 16, 14, 3, 0, 17, 14, 14, 15, 14, 14, 3, 14, 20, 3, 15, 11, 4, 14, 18, 20, 19, 3, 14, 14, 19, 3, 3, 3, 19, 7, 14, 20, 20, 18, 3, 19, 17, 14, 1, 19, 19, 20, 3, 15, 14, 3, 20, 14, 19, 3, 14, 3, 3, 14, 20, 20, 20, 16, 13, 15, 17, 17, 19, 16, 10, 14, 12, 18, 14, 14, 2, 15, 14, 2, 3, 3, 3, ... ]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = topic_model.predict(tweet_holder_processed)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymongo\n",
    "\n",
    "from utils.vector_utils import find_norm , find_similar , vector_from_document_tfidf_dict , vector_from_document_tfidf_mongo\n",
    "from utils.preprocess import pre_process_sentence , pre_process_tokens\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "glove_twitter_vec = np.load('../Data/glove_twiiter_300_vec.npy')\n",
    "glove_twitter_vocab = np.load('../Data/glove_vocab.npy')\n",
    "\n",
    "glove_twitter_dict = dict(zip(glove_twitter_vocab, glove_twitter_vec))\n",
    "\n",
    "client = pymongo.MongoClient()\n",
    "DB_NAME = 'AIQX_MONGO'\n",
    "db = client[DB_NAME]\n",
    "\n",
    "collection_categ_vec = db.get_collection('category_to_vectors')\n",
    "collection = collection_categ_vec.find()\n",
    "all_categ = []\n",
    "for i in collection:\n",
    "    all_categ.append(i[u'category'])\n",
    "    \n",
    "to_remove = [u'Misc. Other',u'Non Profit',u'Tier 2 Services',u'Non Profit', u'Food and Drink', u'Misleading Claims', u'Get Rich Quick']\n",
    "new_categ = [i for i in all_categ if i not in to_remove]\n",
    "\n",
    "pp_dict = {}\n",
    "for categ in new_categ:\n",
    "    pp_dict[categ] = pre_process_tokens(categ).split()\n",
    "word_list = pp_dict.values()\n",
    "word_list = [w for word in word_list for w in word]\n",
    "word_list = list(set(word_list))\n",
    "model_not_know = []\n",
    "for w in word_list:\n",
    "    if w not in glove_twitter_dict:\n",
    "        model_not_know.append(w)\n",
    "print \"Model dont know these words {}\".format(model_not_know)\n",
    "\n",
    "#This is also interesting to try with Ward Hierarchical Clustering\n",
    "\n",
    "tfidf_dict = {k:1.0 for k in word_list}\n",
    "categ_vec_dict = {}\n",
    "for categ, tokens in pp_dict.iteritems():\n",
    "    categ_vec_dict[categ] = vector_from_document_tfidf_dict(tfidf_dict , tokens , glove_twitter_dict)\n",
    "\n",
    "X = categ_vec_dict.values()\n",
    "Y = categ_vec_dict.keys()\n",
    "\n",
    "clusters = MiniBatchKMeans(n_clusters=35, max_iter=1100,batch_size=100,\n",
    "                        n_init=1,init_size=2000)\n",
    "\n",
    "clusters.fit(X)\n",
    "\n",
    "cluster_dict=defaultdict(list)\n",
    "for word,label in zip(Y,clusters.labels_):\n",
    "    cluster_dict[label].append(word)\n",
    "import cPickle\n",
    "cPickle.dump(cluster_dict , open('category_clusters.pkl', 'wb') , protocol = 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_wcp_norm = cPickle.load(open('wcp_topic_frame.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mpoints',\n",
       " 'donate',\n",
       " '#mplaces',\n",
       " 'checked',\n",
       " 'andy',\n",
       " 'home',\n",
       " 'bofa',\n",
       " 'welcome',\n",
       " 'love',\n",
       " 'yass',\n",
       " 'comms',\n",
       " 'like',\n",
       " 'discrimination',\n",
       " 'activists',\n",
       " 'muslim',\n",
       " 'tell',\n",
       " 'talking',\n",
       " 'american',\n",
       " \"today's\",\n",
       " 'winning']"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CATEG_MAT = np.array(categ_vec_dict.values())\n",
    "CATEG_KEY = categ_vec_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a , b = find_similar(find_norm(CATEG_MAT) , find_norm(glove_twitter_dict['today']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(df_wcp_norm['Topic_0'].sort_values(ascending=False)[:20].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_vec = {}\n",
    "for i in xrange(topic_model.num_topics):\n",
    "    words_tweets = list(topic_model.get_topics([i] , 20)['word'])[::-1]\n",
    "    words_tweets = [w.strip('#') for w in words_tweets ]\n",
    "    word_tfidf = {w:i+1 for i, w in enumerate(words_tweets) if w in glove_twitter_dict}\n",
    "    topic_vec[i] = vector_from_document_tfidf_dict( word_tfidf , word_tfidf.keys() , glove_twitter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_vec_wcp = {}\n",
    "for idx ,col_ in enumerate(df_wcp_norm.columns):\n",
    "    words_tweets = list(df_wcp_norm[col_].sort_values(ascending=False)[:20].index[::-1])\n",
    "    words_tweets = [w.strip('#') for w in words_tweets ]\n",
    "    word_tfidf = {w:i+1 for i, w in enumerate(words_tweets) if w in glove_twitter_dict}\n",
    "    topic_vec_wcp[idx] = vector_from_document_tfidf_dict( word_tfidf , word_tfidf.keys() , glove_twitter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "write_file = open('topic_report_wcp.txt', 'wb')\n",
    "\n",
    "for top_ in xrange(topic_model.num_topics):\n",
    "    a , b = find_similar(find_norm(CATEG_MAT) , find_norm(topic_vec_wcp[top_]))\n",
    "    topa = a[:3]\n",
    "    categ = [CATEG_KEY[i] for i in b[:3]]\n",
    "    res = zip(categ,topa)\n",
    "    topwords = list(topic_model.get_topics([top_], 20)['word'])\n",
    "    \n",
    "    write_file.write('Topic_{}'.format(top_))\n",
    "    write_file.write('\\n')\n",
    "    write_file.write('Top_words')\n",
    "    write_file.write('\\n')\n",
    "    write_file.write('___________')\n",
    "    write_file.write('\\n')\n",
    "    write_file.write(str(topwords))\n",
    "    write_file.write('\\n')\n",
    "    write_file.write('Top Categories')\n",
    "    write_file.write('\\n')\n",
    "    write_file.write('___________')\n",
    "    write_file.write('\\n')\n",
    "    write_file.write(str(res))\n",
    "    write_file.write('\\n')\n",
    "    write_file.write('****************** \\n')\n",
    "    write_file.write('\\n')\n",
    " \n",
    "write_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "write_file = open('topic_report_wcp.txt', 'wb')\n",
    "\n",
    "for idx ,col_ in enumerate(df_wcp_norm.columns):\n",
    "    a , b = find_similar(find_norm(CATEG_MAT) , find_norm(topic_vec_wcp[idx]))\n",
    "    topa = a[:3]\n",
    "    categ = [CATEG_KEY[i] for i in b[:3]]\n",
    "    res = zip(categ,topa)\n",
    "    topwords = list(df_wcp_norm[col_].sort_values(ascending=False)[:20].index)\n",
    "    \n",
    "    write_file.write('Topic_{}'.format(idx))\n",
    "    write_file.write('\\n')\n",
    "    write_file.write('Top_words')\n",
    "    write_file.write('\\n')\n",
    "    write_file.write('___________')\n",
    "    write_file.write('\\n')\n",
    "    write_file.write(str(topwords))\n",
    "    write_file.write('\\n')\n",
    "    write_file.write('Top Categories')\n",
    "    write_file.write('\\n')\n",
    "    write_file.write('___________')\n",
    "    write_file.write('\\n')\n",
    "    write_file.write(str(res))\n",
    "    write_file.write('\\n')\n",
    "    write_file.write('****************** \\n')\n",
    "    write_file.write('\\n')\n",
    " \n",
    "write_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "tweet_extended = [t for tweet in tweet_preprocess for t in tweet ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter = Counter(tweet_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = sorted ( counter.items() , key = lambda x: x[1] , reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "words_tweets = [a for a,b in w]\n",
    "words_tweets = [w.strip('#') for w in words_tweets ]\n",
    "word_tfidf = {w:1.0 for i, w in enumerate(words_tweets) if w in glove_twitter_dict}\n",
    "trend_vec = vector_from_document_tfidf_dict( word_tfidf , word_tfidf.keys() , glove_twitter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a , b = find_similar(find_norm(CATEG_MAT) , find_norm(trend_vec))\n",
    "topa = a[:10]\n",
    "categ = [CATEG_KEY[i] for i in b[:10]]\n",
    "res = zip(categ,topa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Sports and Events', 0.83642864),\n",
       " (u'Home and Decor', 0.72825092),\n",
       " (u'Fitness and Health', 0.72267926),\n",
       " (u'Video Games', 0.72193456),\n",
       " (u'Sports', 0.7170397),\n",
       " (u'Family and Parenting', 0.71623802),\n",
       " (u'Online Games', 0.67085707),\n",
       " (u'Home&Garden', 0.66232729),\n",
       " (u'Tobacco and Smoking Products', 0.64986849),\n",
       " (u'Toys/Games', 0.64572543)]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = {}\n",
    "for w in word_tfidf:\n",
    "    a , b = find_similar(find_norm(CATEG_MAT) , find_norm(glove_twitter_dict[w]))\n",
    "    res[w] = (CATEG_KEY[b[0]], a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'bolt': (u'Sports and Events', 0.35478362),\n",
       " u'canada': (u'Travel Services', 0.55970305),\n",
       " u'colorado': (u'Real Estate', 0.46520364),\n",
       " u'football': (u'Sports', 0.77181947),\n",
       " u'ford': (u'Automotive and Vehicles', 0.55983382),\n",
       " u'game': (u'Video Games', 0.79502982),\n",
       " u'gold': (u'Jewelry', 0.52809495),\n",
       " u'like': (u'Home and Decor', 0.72784287),\n",
       " u'mercedes': (u'Automotive and Vehicles', 0.43572327),\n",
       " u'paralympics': (u'Sports and Events', 0.37950128),\n",
       " u'penny': (u'Deals and Coupons', 0.38812646),\n",
       " u'rugby': (u'Sports', 0.63019615),\n",
       " u'season': (u'Sports and Events', 0.62155765),\n",
       " u'spark': (u'Automotive and Vehicles', 0.41202879),\n",
       " u'swimming': (u'Sports and Events', 0.50748211),\n",
       " u'team': (u'Sports and Events', 0.61411911),\n",
       " u'time': (u'Home and Decor', 0.67333037),\n",
       " u'today': (u'Sports and Events', 0.76472807),\n",
       " u'usain': (u'Sports and Events', 0.28603327)}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a , b = find_similar(find_norm(glove_twitter_vec) , find_norm(glove_twitter_dict['corbyn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corbyn',\n",
       " 'caelan',\n",
       " 'shada',\n",
       " 'moosey',\n",
       " 'brigette',\n",
       " 'kersten',\n",
       " 'zharfan',\n",
       " 'ruairi',\n",
       " 'daylen',\n",
       " '\\xd8\\xad\\xd9\\x81\\xd9\\x84\\xd9\\x86\\xd8\\xa7']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[glove_twitter_vocab[i] for i in b[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28977734"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(find_norm(glove_twitter_dict['train']) , find_norm(glove_twitter_dict['events']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################### Topic Vec ---> Vec Relationship\n",
    "\n",
    "topic_vec_keys = topic_vec.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_vec_values = np.array(topic_vec.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a , b = find_similar(find_norm(np.array(topic_vec_wcp.values())) , find_norm(topic_vec_wcp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  0.80880982,  0.77605438,  0.74838299,  0.7306779 ,\n",
       "        0.72602791,  0.72203177,  0.70262933,  0.68319756,  0.67462134,\n",
       "        0.67155147,  0.65937942,  0.65381235,  0.64274293,  0.64195365,\n",
       "        0.63957697,  0.62918758,  0.62296802,  0.59599638,  0.55857086,\n",
       "        0.55492938], dtype=float32)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 15, 19, 18,  3,  1, 17, 16,  8, 14,  5,  7, 11, 20, 12,  9, 13,\n",
       "        2,  6,  4, 10])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "digital       0.000025\n",
       "#jobs         0.000025\n",
       "click         0.000025\n",
       "#job          0.000025\n",
       "latest        0.000025\n",
       "tech          0.000025\n",
       "cloud         0.000025\n",
       "#careerarc    0.000025\n",
       "#delljobs     0.000025\n",
       "we're         0.000025\n",
       "Name: Topic_15, dtype: float64"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wcp_norm['Topic_15'].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#ebay            0.000025\n",
       "#maga            0.000025\n",
       "engine           0.000025\n",
       "#reviews         0.000025\n",
       "#ideas           0.000025\n",
       "steel            0.000025\n",
       "#trump           0.000025\n",
       "#neverhillary    0.000025\n",
       "#trumptrain      0.000025\n",
       "#events          0.000025\n",
       "#trump2016       0.000025\n",
       "#apps            0.000025\n",
       "#hillary         0.000025\n",
       "#news            0.000025\n",
       "#america         0.000025\n",
       "brownlee         0.000025\n",
       "hamilton         0.000025\n",
       "#ohio            0.000025\n",
       "surf             0.000025\n",
       "#ebayus          0.000025\n",
       "#dncleak         0.000025\n",
       "#giveaway        0.000025\n",
       "#foxnews         0.000025\n",
       "#products        0.000025\n",
       "pairs            0.000025\n",
       "pump             0.000025\n",
       "motorcycle       0.000025\n",
       "#deplorables     0.000025\n",
       "#clinton         0.000025\n",
       "stereo           0.000025\n",
       "                   ...   \n",
       "airport          0.000009\n",
       "help             0.000009\n",
       "love             0.000009\n",
       "hunger           0.000009\n",
       "want             0.000009\n",
       "#smallbiz        0.000009\n",
       "core             0.000009\n",
       "free             0.000009\n",
       "#weightloss      0.000009\n",
       "check            0.000009\n",
       "good             0.000009\n",
       "deals            0.000009\n",
       "great            0.000009\n",
       "best             0.000009\n",
       "football         0.000009\n",
       "people           0.000009\n",
       "fashion          0.000009\n",
       "time             0.000009\n",
       "don't            0.000009\n",
       "week             0.000009\n",
       "today            0.000009\n",
       "friend           0.000009\n",
       "baby             0.000009\n",
       "clinton          0.000009\n",
       "it's             0.000009\n",
       "verizon          0.000009\n",
       "dell             0.000009\n",
       "hillary          0.000009\n",
       "like             0.000009\n",
       "trump            0.000009\n",
       "Name: Topic_0, dtype: float64"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wcp_norm['Topic_0'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
